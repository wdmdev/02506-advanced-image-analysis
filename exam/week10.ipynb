{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 10 Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a  convolutional neural network (activation not drawn)\n",
    "<p align=\"center\">\n",
    "    <img src=\"CNN.png\"/>\n",
    "</p>\n",
    "used for grayscale images of size 120 × 120 pixels. We follow the convention from VGG19 paper, meaning that conv5-4 uses 5 × 5 kernels and has 4 channels. Unlike VGG19, we do not use spatial padding of the input, and output is restricted to voxels where kernel lies entirely withing the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1)\n",
    "What is the size (total number of neurons) of the output of the network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "460488"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_image = 120*120\n",
    "\n",
    "# 2*4 because we lose 4 pixels on x and 4 on y from using a 5x5 conv without padding\n",
    "# * 4 because we have 4 channels in the conv\n",
    "conv_5_4_size = (initial_image - (2*4)) * 4\n",
    "\n",
    "conv_5_8_size = (conv_5_4_size - (2*4)) * 8\n",
    "\n",
    "#+8 becaues we have 8 bias nodes in the last layer, one for each channel in the conv\n",
    "conv_5_8_size + 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2)\n",
    "How many learnable paremeters are there in all? Remember to consider the biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "460492"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can reuse the size from before and just add 4, for the 4 bias nodes in the first conv\n",
    "conv_5_8_size + 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3)\n",
    "We add max pooling and downscaling over a 2 × 2 window after each of the convolutional layers. What is the size of the output now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each max pooling will half the size of the convoluted image\n",
    "max_pool1_size = conv_5_4_size/2\n",
    "max_pool2_size = max_pool1_size/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14400.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Again +8 for the 8 bias nodes in the last conv\n",
    "max_pool2_size + 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "85efbad99586510d90dc313248aef6c49ee289f9e88fd2f9e5e83e811600078a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit ('adv-img': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
